{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "previous-cheat",
   "metadata": {},
   "source": [
    "# NLP_GoingDeeper | P05.Transformer_Translation_Kor2Eng\n",
    "---\n",
    "- 한국어 문장을 입력으로 하여 영어로 번역된 문장을 출력하는 번역기를 만들어봅니다. \n",
    "\n",
    "- [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)에서 제공하는 데이터셋을 활용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ln -s ~/data ~/aiffel/GoingDeeper/DATA/transformer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "another-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fonts-nanum is already the newest version (20170925-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "#- 나눔글꼴 설치\n",
    "! sudo apt -qq -y install fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expected-elevation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#- SentencePiece 설치\n",
    "\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frank-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/user/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic')\n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "polar-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/GoingDeeper/DATA/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "likely-geometry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size:(94123,94123)\n",
      "Example:\n",
      ">>\n",
      "한국어 : 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      "영어 : Much of personal computing is about \"can you top this?\"\n",
      "\n",
      ">>\n",
      "한국어 : 모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\n",
      "영어 : so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.\n",
      "\n",
      ">>\n",
      "한국어 : 그러나 이것은 또한 책상도 필요로 하지 않는다.\n",
      "영어 : Like all optical mice, But it also doesn't need a desk.\n",
      "\n",
      ">>\n",
      "한국어 : 79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다.\n",
      "영어 : uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.\n",
      "\n",
      ">>\n",
      "한국어 : 정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.\n",
      "영어 : Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "print(\"Data Size:({},{})\".format(len(kor), len(eng)))\n",
    "print(\"Example:\")\n",
    "\n",
    "cnt = 0\n",
    "for ko, en in zip(kor, eng):\n",
    "    print(f\">>\\n한국어 : {ko}\\n영어 : {en}\\n\")\n",
    "    cnt += 1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-juice",
   "metadata": {},
   "source": [
    "# 2. 데이터 정제 및 토큰화 \n",
    "### 1) 중복데이터 제거\n",
    "- set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거합니다. \n",
    "- 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "central-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor, eng):\n",
    "    assert len(kor) == len(eng)\n",
    "    cleaned_corpus = list(set(zip(kor, eng)))\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corrected-taste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = clean_corpus(kor, eng)\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-utilization",
   "metadata": {},
   "source": [
    "### 2) 정제함수 정의\n",
    "\n",
    "- 모든 입력을 소문자로 변환합니다.\n",
    "- 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "- 문장부호 양옆에 공백을 추가합니다.\n",
    "- 문장 앞뒤의 불필요한 공백을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "portable-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() #- 소문자 변환\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣?.!,]+\", \" \", sentence) #- 알파벳, 문장부호, 한글 이외는 제거\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #- 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "    sentence = sentence.strip() #- 양쪽 공백 제거\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-precipitation",
   "metadata": {},
   "source": [
    "### 3) 토큰화\n",
    "- 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행합니다. \n",
    "- 토큰화에는 [SentencePiece](https://github.com/google/sentencepiece)를 사용합니다. \n",
    "    - generate_tokenizer() 함수를 정의하여 최종적으로 ko_tokenizer와 en_tokenizer를 얻습니다. \n",
    "    - en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게 합니다.\n",
    "    - 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    "    - 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
    "    - 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "        - \\<PAD> : 0 / \\<BOS\\> : 1 / \\<EOS\\> : 2 / \\<UNK\\> : 3\n",
    "- [spm의 입력은 txt로 넣어야 합니다.](https://lovit.github.io/nlp/2018/04/02/wpm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "governmental-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus, model_type='unigram', vocab_size=32000, lang=\"ko\"): \n",
    "    \n",
    "    input_file = f'{lang}_spm_input.txt'\n",
    "\n",
    "    with open(input_file, 'w', encoding='utf-8') as f:\n",
    "        for sent in corpus:\n",
    "            f.write('{}\\n'.format(sent))\n",
    "        \n",
    "    sp_model_root='sentencepiece'\n",
    "    if not os.path.isdir(sp_model_root): os.mkdir(sp_model_root)\n",
    "    \n",
    "    vocab_size = vocab_size # vocab 사이즈\n",
    "    prefix = 'tokenizer_%s_%s' % (lang,model_type+str(vocab_size))\n",
    "    prefix = os.path.join(sp_model_root, prefix) # 저장될 tokenizer 모델에 붙는 이름\n",
    "    pad_id=0\n",
    "    bos_id=1 #<start> token을 1으로 설정\n",
    "    eos_id=2 #<end> token을 2으로 설정\n",
    "    unk_id=3 #<unknown> token을 3으로 설정\n",
    "    character_coverage = 1.0 # to reduce character set \n",
    "    # model_type = model_type # Choose from unigram (default), bpe, char, or word\n",
    "    input_argument = '--input=%s --pad_id=%s --bos_id=%s --eos_id=%s --unk_id=%s --model_prefix=%s --vocab_size=%s --character_coverage=%s --model_type=%s'\n",
    "    cmd = input_argument%(input_file, pad_id, bos_id, eos_id, unk_id, prefix, vocab_size, character_coverage, model_type)\n",
    "\n",
    "#     cmd = templates.format(corpus,\n",
    "#                 pad_id,\n",
    "#                 bos_id,\n",
    "#                 eos_id,\n",
    "#                 unk_id,\n",
    "#                 prefix,\n",
    "#                 vocab_size,\n",
    "#                 character_coverage,\n",
    "#                 model_type)\n",
    "                       \n",
    "    spm.SentencePieceTrainer.Train(cmd)\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'{prefix}.model')\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legendary-stockholm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('그는 당시 자녀양육비를 내지 못해 경찰에 수배 중이었으며 경찰은 그의 소재를 파악하지 못하고 있었다.',\n",
       " 'He was being sought on a parole violation for failure to pay child support, but police who have said they want to question him further had been unable to locate him.')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pursuant-paper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 32000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0],pair[1]\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, 'unigram', SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, 'unigram', TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "perceived-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_test_txts = ['세계기상기구는 “코로나19에 따른 경제 활동 둔화로 탄소 배출이 일시적으로 줄었지만, 대기 중 이산화탄소 농도에 뚜렷한 영향을 주지 못했다”고 설명했다.','기후변화 대응·적응에 필요한 재원 분담 과정에서 나타나는 선진국과 개발도상국의 이견도 오래 묵은 논쟁거리다.']\n",
    "en_test_txts = ['Democrats, including President Biden, are lobbying for Senator Joe Manchin’s support, knowing he is a crucial swing vote on their domestic agenda.','The contest to fill a vacant State House seat in South Texas has exposed the vulnerabilities of a Democratic stronghold.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "european-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSP(sp, txts):\n",
    "    tokens_list = []\n",
    "    for txt in txts:\n",
    "        tokens = sp.encode_as_pieces(txt)\n",
    "        #ids = sp.encode_as_ids(txt)\n",
    "        tokens_list.append(tokens)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "exclusive-apparatus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁세계',\n",
       "  '기',\n",
       "  '상',\n",
       "  '기구',\n",
       "  '는',\n",
       "  '▁',\n",
       "  '“',\n",
       "  '코',\n",
       "  '로',\n",
       "  '나',\n",
       "  '19',\n",
       "  '에',\n",
       "  '▁따른',\n",
       "  '▁경제',\n",
       "  '▁활동',\n",
       "  '▁둔화',\n",
       "  '로',\n",
       "  '▁탄소',\n",
       "  '▁배출',\n",
       "  '이',\n",
       "  '▁일시적',\n",
       "  '으로',\n",
       "  '▁줄',\n",
       "  '었지만',\n",
       "  ',',\n",
       "  '▁대기',\n",
       "  '▁중',\n",
       "  '▁이산화탄소',\n",
       "  '▁농도',\n",
       "  '에',\n",
       "  '▁뚜렷',\n",
       "  '한',\n",
       "  '▁영향',\n",
       "  '을',\n",
       "  '▁주지',\n",
       "  '▁못했다',\n",
       "  '”',\n",
       "  '고',\n",
       "  '▁설명했다',\n",
       "  '.'],\n",
       " ['▁기후변화',\n",
       "  '▁대응',\n",
       "  '·',\n",
       "  '적',\n",
       "  '응',\n",
       "  '에',\n",
       "  '▁필요한',\n",
       "  '▁재원',\n",
       "  '▁분담',\n",
       "  '▁과정에서',\n",
       "  '▁나타나',\n",
       "  '는',\n",
       "  '▁선진국',\n",
       "  '과',\n",
       "  '▁개발도상국',\n",
       "  '의',\n",
       "  '▁이견',\n",
       "  '도',\n",
       "  '▁오래',\n",
       "  '▁묵',\n",
       "  '은',\n",
       "  '▁논쟁거리',\n",
       "  '다',\n",
       "  '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSP(ko_tokenizer, ko_test_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "another-diary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  '▁',\n",
       "  'D',\n",
       "  'em',\n",
       "  'ocrats',\n",
       "  ',',\n",
       "  '▁including',\n",
       "  '▁',\n",
       "  'P',\n",
       "  'res',\n",
       "  'ident',\n",
       "  '▁',\n",
       "  'B',\n",
       "  'iden',\n",
       "  ',',\n",
       "  '▁are',\n",
       "  '▁lobby',\n",
       "  'ing',\n",
       "  '▁for',\n",
       "  '▁',\n",
       "  'S',\n",
       "  'en',\n",
       "  'ator',\n",
       "  '▁',\n",
       "  'J',\n",
       "  'o',\n",
       "  'e',\n",
       "  '▁',\n",
       "  'M',\n",
       "  'an',\n",
       "  'chi',\n",
       "  'n',\n",
       "  '’',\n",
       "  's',\n",
       "  '▁support',\n",
       "  ',',\n",
       "  '▁knowing',\n",
       "  '▁he',\n",
       "  '▁is',\n",
       "  '▁a',\n",
       "  '▁crucial',\n",
       "  '▁swing',\n",
       "  '▁vote',\n",
       "  '▁on',\n",
       "  '▁their',\n",
       "  '▁domestic',\n",
       "  '▁agenda',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " ['<s>',\n",
       "  '▁',\n",
       "  'T',\n",
       "  'he',\n",
       "  '▁contest',\n",
       "  '▁to',\n",
       "  '▁fill',\n",
       "  '▁a',\n",
       "  '▁vacant',\n",
       "  '▁',\n",
       "  'S',\n",
       "  'tate',\n",
       "  '▁',\n",
       "  'H',\n",
       "  'ouse',\n",
       "  '▁seat',\n",
       "  '▁in',\n",
       "  '▁',\n",
       "  'S',\n",
       "  'out',\n",
       "  'h',\n",
       "  '▁',\n",
       "  'T',\n",
       "  'ex',\n",
       "  'as',\n",
       "  '▁has',\n",
       "  '▁exposed',\n",
       "  '▁the',\n",
       "  '▁vulnerabilities',\n",
       "  '▁of',\n",
       "  '▁a',\n",
       "  '▁',\n",
       "  'D',\n",
       "  'e',\n",
       "  'mo',\n",
       "  'cratic',\n",
       "  '▁stronghold',\n",
       "  '.',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSP(en_tokenizer, en_test_txts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-speed",
   "metadata": {},
   "source": [
    "### 4) 데이터 선별\n",
    "- 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축합니다. \n",
    "- 이후 텐서 enc_train 과 dec_train 으로 변환합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    m\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-boost",
   "metadata": {},
   "source": [
    "# 3. 모델 설계\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-imperial",
   "metadata": {},
   "source": [
    "# 4. 훈련\n",
    "### 1) 모델 선언\n",
    "2 Layer를 가지는 Transformer를 선언합니다. (하이퍼파라미터는 자유롭게 조절) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-heath",
   "metadata": {},
   "source": [
    "### 2) Learning Rate Scheduler\n",
    "- 논문에서 사용한 것과 동일한 Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = # [[YOUR CODE]]\n",
    "optimizer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-release",
   "metadata": {},
   "source": [
    "### 3) Loss 함수 정의\n",
    "- Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, Masking 되지 않은 입력의 개수로 Scaling하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-legislature",
   "metadata": {},
   "source": [
    "### 4) train_step 함수 정의\n",
    "- 입력 데이터에 알맞은 Mask를 생성하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-classroom",
   "metadata": {},
   "source": [
    "### 5) 학습 진행\n",
    "- 매 Epoch 마다 제시된 예문에 대한 번역을 생성하고, 멋진 번역이 생성되면 그때의 하이퍼파라미터와 생성된 번역을 기록합니다. \n",
    "\n",
    "- 예문\n",
    "    1. 오바마는 대통령이다.\n",
    "    2. 시민들은 도시 속에 산다.\n",
    "    3. 커피는 필요 없다.\n",
    "    4. 일곱 명의 사망자가 발생했다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "\n",
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)\n",
    "Copyright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-criticism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "developing-williams",
   "metadata": {},
   "source": [
    "# 4. 결과\n",
    "### Translations\n",
    "1. 오바마는 대통령이다. > \n",
    "2. 시민들은 도시 속에 산다. > \n",
    "3. 커피는 필요 없다. > \n",
    "4. 일곱 명의 사망자가 발생했다. > \n",
    "\n",
    "### Hyperparameters\n",
    "- n_layers: \n",
    "- d_model: \n",
    "- n_heads: \n",
    "- d_ff: \n",
    "- dropout: \n",
    "\n",
    "### Training Parameters\n",
    "- Warmup Steps: \n",
    "- Batch Size: \n",
    "- Epoch At: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-operator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
